# -------------------------------------------------------------------
# main_HPO.py
#
# ä½¿ç”¨ Optuna (è²è‘‰æ–¯å„ªåŒ–) ä¾†è‡ªå‹•æœå°‹ 
# TransformerAutoencoder (TAE) çš„æœ€ä½³è¶…åƒæ•¸ã€‚
#
# ä¾è³´æ–¼: Transformer_model.py, optuna
# -------------------------------------------------------------------

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np
import os
import optuna # å°å…¥ Optuna
from optuna.exceptions import TrialPruned # ç”¨æ–¼å‰ªæ
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split # å°å…¥åˆ†å‰²å·¥å…·
# -------------------------------------------------------------------
# 1. å¾æˆ‘å€‘ç¾æœ‰çš„ Transformer_model.py ä¸­å°å…¥æ§‹å»ºæ¨¡å¡Š
#    (è«‹ç¢ºä¿æ­¤è…³æœ¬èˆ‡ Transformer_model.py åœ¨åŒä¸€è³‡æ–™å¤¾ä¸­)
# -------------------------------------------------------------------
try:
    from Transformer_model import (
        TransformerAutoencoder, 
        create_dataloader
    )
except ImportError:
    print("éŒ¯èª¤: æ‰¾ä¸åˆ° Transformer_model.py")
    print("è«‹ç¢ºä¿ main_HPO.py èˆ‡ Transformer_model.py æ”¾åœ¨åŒä¸€å€‹è³‡æ–™å¤¾ä¸­ã€‚")
    exit(1)

# -------------------------------------------------------------------
# 2. å…¨å±€è®Šé‡ (ç”¨æ–¼ HPO)
# -------------------------------------------------------------------
FOLDER = "../data_process/data_pack"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 64     # HPO æ™‚å¯ä»¥ä½¿ç”¨ç¨å¤§çš„ Batch Size
N_EPOCHS_PER_TRIAL = 50 # ç‚ºäº†å¿«é€Ÿè¿­ä»£ï¼Œæˆ‘å€‘å…ˆç”¨ 50 epochs (ä¹‹å¾Œå¯æ”¹å› 100)
N_TRIALS = 500       # ç¸½å…±è¦å˜—è©¦ 50 ç¨®ä¸åŒçš„è¶…åƒæ•¸çµ„åˆ


# -------------------------------------------------------------------
# 3. Objective (ç›®æ¨™å‡½æ•¸)
#    Optuna æœƒä¸æ–·èª¿ç”¨æ­¤å‡½æ•¸ï¼Œä¸¦è©¦åœ–æœ€å°åŒ–å®ƒçš„å›å‚³å€¼ã€‚
# -------------------------------------------------------------------

def objective(trial: optuna.trial.Trial) -> float:
    """
    Optuna çš„ç›®æ¨™å‡½æ•¸ã€‚
    1. å»ºè­°ä¸€çµ„è¶…åƒæ•¸ã€‚
    2. å»ºç«‹ä¸¦è¨“ç·´ TAE æ¨¡å‹ã€‚
    3. å›å‚³æœ€ä½³çš„ *validation_loss*ã€‚ (å·²ä¿®æ­£ Data Snooping)
    """
    
    # --- A. å®šç¾©è¶…åƒæ•¸çš„ã€Œæœå°‹ç©ºé–“ã€ ---
    # (é€™éƒ¨åˆ†ä¿æŒä¸è®Š)
    
    # 1. å­¸ç¿’ç‡ (log å°ºåº¦)
    lr = trial.suggest_float("lr", 1e-5, 1e-3, log=True)
    
    # 2. æ¶æ§‹åƒæ•¸
    d_model = trial.suggest_categorical("d_model", [32, 64, 128, 256])
    nhead = trial.suggest_categorical("nhead", [2, 4, 8])
    num_encoder_layers = trial.suggest_int("num_encoder_layers", 2, 6)
    num_decoder_layers = num_encoder_layers # ä¿æŒå°ç¨±
    dim_feedforward = trial.suggest_categorical("dim_feedforward", [64, 128, 256, 512])

    # 3. æª¢æŸ¥ç´„æŸï¼šnhead å¿…é ˆèƒ½æ•´é™¤ d_model
    if d_model % nhead != 0:
        print(f"Pruning trial: d_model={d_model} % nhead={nhead} != 0")
        raise TrialPruned()

    print(f"\n--- [Trial {trial.number}] ---")
    print(f"Params: lr={lr:.2e}, d_model={d_model}, nhead={nhead}, num_layers={num_encoder_layers}, dim_ff={dim_feedforward}")

    # --- B. è¨­ç½®æ¨¡å‹å’Œæ•¸æ“š (!!! å·²ä¿®æ”¹ï¼šåˆ†å‰²é©—è­‰é›† !!!) ---
    
    # 1. åŠ è¼‰ã€Œå®Œæ•´ã€çš„è¨“ç·´æ•¸æ“šé›†
    #    (shuffle=False ç¢ºä¿æ¯æ¬¡ HPO çš„åˆ†å‰²éƒ½ä¸€è‡´)
    _, full_train_dataset = create_dataloader(FOLDER, "post_vol_", "train", batch_size=BATCH_SIZE, shuffle=False, compute_stats=False)

    # 2. å°‡ã€Œè¨“ç·´é›†ã€åˆ†å‰²ç‚ºã€Œæ–°è¨“ç·´é›†ã€(80%) å’Œã€Œé©—è­‰é›†ã€(20%)
    train_indices, val_indices = train_test_split(
        range(len(full_train_dataset)), 
        test_size=0.2, 
        random_state=42 # å›ºå®š random_state ç¢ºä¿å¯é‡ç¾æ€§
    )
    
    train_subset = Subset(full_train_dataset, train_indices)
    val_subset = Subset(full_train_dataset, val_indices)
    
    # 3. å‰µå»ºæ–°çš„ Dataloader
    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)
    validation_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False) # é©—è­‰é›†ä¸éœ€è¦ shuffle

    # 4. ç²å–å½¢ç‹€ (å¾æ–°çš„ train_loader)
    first_batch, _, _ = next(iter(train_loader))
    seq_len = first_batch.shape[2] # 41
    input_dim = first_batch.shape[3] # 20
    
    # 5. æ ¹æ“šå»ºè­°çš„è¶…åƒæ•¸å»ºç«‹æ¨¡å‹ (èˆ‡ä¹‹å‰ç›¸åŒ)
    model = TransformerAutoencoder(
        input_dim=input_dim,
        seq_len=seq_len,
        latent_dim=10, # æˆ‘å€‘å›ºå®š latent_dim=10
        d_model=d_model,
        nhead=nhead,
        num_encoder_layers=num_encoder_layers,
        num_decoder_layers=num_decoder_layers,
        dim_feedforward=dim_feedforward
    ).to(DEVICE)
    
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = CosineAnnealingLR(optimizer, T_max=N_EPOCHS_PER_TRIAL, eta_min=lr * 0.01)

    best_validation_loss = float('inf') # (!!! å·²ä¿®æ”¹ !!!)

    # --- C. è¨“ç·´è¿´åœˆ (!!! å·²ä¿®æ”¹ï¼šä½¿ç”¨é©—è­‰é›† !!!) ---
    for epoch in range(N_EPOCHS_PER_TRIAL):
        model.train()
        total_train_loss = 0.0
        # (è¨“ç·´è¿´åœˆ ... ä¿æŒä¸è®Š)
        for x, _, _ in train_loader:
            x = x.to(DEVICE)
            optimizer.zero_grad()
            x_recon = model(x)
            loss = F.mse_loss(x_recon, x)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_train_loss += loss.item() * x.size(0)
        
        scheduler.step()
        # avg_train_loss = total_train_loss / len(train_loader.dataset) # æ‡‰ç‚º len(train_subset)
        
        # è©•ä¼°ã€Œé©—è­‰é›†ã€ (!!! å·²ä¿®æ”¹ !!!)
        model.eval()
        total_validation_loss = 0.0
        with torch.no_grad():
            for x_val, _, _ in validation_loader: # (!!! å·²ä¿®æ”¹ !!!)
                x_val = x_val.to(DEVICE)
                x_recon_val = model(x_val)
                loss_val = F.mse_loss(x_recon_val, x_val)
                total_validation_loss += loss_val.item() * x_val.size(0)
        
        avg_validation_loss = total_validation_loss / len(val_subset) # (!!! å·²ä¿®æ”¹ !!!)
        
        if avg_validation_loss < best_validation_loss:
            best_validation_loss = avg_validation_loss
        
        # (!!! å·²ä¿®æ”¹ !!!)
        print(f"Trial {trial.number}, Epoch {epoch+1}/{N_EPOCHS_PER_TRIAL} | val_loss={avg_validation_loss:.6f}")

        # --- D. é—œéµçš„ã€Œå‰ªæã€æ­¥é©Ÿ (!!! å·²ä¿®æ”¹ !!!) ---
        
        # 1. å‘ Optuna å ±å‘Šç›®å‰ epoch çš„ *validation_loss*
        trial.report(avg_validation_loss, epoch) # (!!! å·²ä¿®æ”¹ !!!)
        
        # 2. æª¢æŸ¥ Optuna æ˜¯å¦èªç‚ºè¿™ä¸ª trial å·²ç¶“æ²’å¸Œæœ›äº†
        if trial.should_prune():
            print(f"Pruning trial {trial.number} at epoch {epoch+1} due to poor performance.")
            raise TrialPruned()

    # è¿´åœˆçµæŸå¾Œï¼Œå›å‚³æ­¤ trial é”åˆ°çš„ã€Œæœ€ä½³ validation_lossã€
    return best_validation_loss # (!!! å·²ä¿®æ”¹ !!!)

# -------------------------------------------------------------------
# 4. ä¸»åŸ·è¡Œå‡½æ•¸
# -------------------------------------------------------------------
def run_hpo():
    print(f"é–‹å§‹ Optuna HPO ({N_TRIALS} trials, {N_EPOCHS_PER_TRIAL} epochs/trial)...")
    print(f"å°‡åœ¨ {DEVICE} ä¸Šé‹è¡Œ")
    
    # è¨­ç½®ä¸€å€‹ã€Œå„²å­˜åº«ã€ï¼ŒOptuna æœƒå°‡çµæœä¿å­˜åœ¨ä¸€å€‹ .db æª”æ¡ˆä¸­
    # é€™æ¨£å°±ç®—ç¨‹å¼ä¸­æ–·ï¼Œä¹Ÿå¯ä»¥å¾ä¸Šæ¬¡çš„åœ°æ–¹ç¹¼çºŒ
    storage_name = "sqlite:///tae_hpo.db"
    
    # 1. å‰µå»º Study (ç ”ç©¶)
    # TPE (Tree-structured Parzen Estimator) æ˜¯ Optuna é è¨­çš„è²è‘‰æ–¯å„ªåŒ–ç®—æ³•
    # Pruner æœƒè‡ªå‹•å‰ªæ‰ã€Œçœ‹èµ·ä¾†æ²’å¸Œæœ›ã€çš„ trial
    study = optuna.create_study(
        study_name="tae-hpo-v1",
        storage=storage_name,
        load_if_exists=True, # å¦‚æœ .db æª”æ¡ˆå­˜åœ¨ï¼Œå°±åŠ è¼‰ä¸¦ç¹¼çºŒ
        direction="minimize",  # æˆ‘å€‘çš„ç›®æ¨™æ˜¯ã€Œæœ€å°åŒ–ã€test_loss
        pruner=optuna.pruners.MedianPruner() # ä½¿ç”¨ã€Œä¸­ä½æ•¸å‰ªæå™¨ã€
    )
    
    # 2. é–‹å§‹å„ªåŒ–ï¼
    # é€™æœƒé‹è¡Œ N_TRIALS æ¬¡ objective å‡½æ•¸
    try:
        study.optimize(objective, n_trials=N_TRIALS)
    except KeyboardInterrupt:
        print("ä½¿ç”¨è€…æ‰‹å‹•åœæ­¢ HPOã€‚")
    
    # 3. æ‰“å°æœ€ä½³çµæœ
    print("\n--- [HPO å®Œæˆ] ---")
    print(f"ç¸½å…±å®Œæˆçš„ Trial æ•¸é‡: {len(study.trials)}")
    
    best_trial = study.best_trial
    print("\nğŸ‰ æœ€ä½³ Trial æ‰¾åˆ°äº† ğŸ‰")
    print(f"  Trial ç·¨è™Ÿ: {best_trial.number}")
    print(f"  æœ€ä½³ Test Loss (MSE): {best_trial.value:.8f}")
    
    print("\n  æœ€ä½³è¶…åƒæ•¸ (Hyperparameters):")
    for key, value in best_trial.params.items():
        print(f"    - {key}: {value}")

    print("\n--- å¦‚ä½•ä½¿ç”¨é€™äº›åƒæ•¸ ---")
    print("1. æ‰“é–‹ main_Transformer.py")
    print(f"2. ä¿®æ”¹ train_and_save_TAE å‡½æ•¸çš„ lr={best_trial.params['lr']:.2e}")
    print(f"3. ä¿®æ”¹ TransformerAutoencoder çš„ __init__ é»˜èªå€¼:")
    print(f"     d_model={best_trial.params['d_model']}")
    print(f"     nhead={best_trial.params['nhead']}")
    print(f"     num_encoder_layers={best_trial.params['num_encoder_layers']}")
    print(f"     num_decoder_layers={best_trial.params['num_encoder_layers']}")
    print(f"     dim_feedforward={best_trial.params['dim_feedforward']}")
    print("4. é‡æ–°é‹è¡Œ main_Transformer.py (if 1) ä¾†è¨“ç·´æœ€çµ‚çš„æœ€ä½³æ¨¡å‹ã€‚")
    
    print("\n(å¯é¸) é‹è¡Œè¦–è¦ºåŒ–å„€è¡¨æ¿:")
    print(f"optuna-dashboard {storage_name}")


if __name__ == "__main__":
    run_hpo()